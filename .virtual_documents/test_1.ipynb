


import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow import keras
import keras.backend as K
import re
import tiktoken # Byte Pair Encoding 





dataset = tfds.load(name='tiny_shakespeare')

train = dataset['train']
for text in train:
    x = text['text'].numpy().decode('utf-8')
print(x[:100])
print(f"\nlength of the entire text file: {len(x)}")





tokens = re.split(r'([,.:;?_!"()\']|--|\s)', x)
print(tokens[:100])

vocabulary = sorted(set(tokens))

# Create a Dictionary with additional special tokens ("<|unk|>", "<|eos|>") 
# for an unkown word or the end of text (incase I train with multiple text sources).\

dictionary = {item:value for value, item in enumerate(vocabulary)}
dictionary["<|unk|>"] = len(dictionary)
dictionary["<|eos|>"] = len(dictionary)
#dictionary["<|bos|>"] = len(dictionary)
#dictionary["<|pad|>"] = len(dictionary)


class SimpleTockenizer:
    """
    A simple tokenizer which using a dictionary converts the text into token ids 
    """
    def __init__(self, dictionary):
        self.dictionary = dictionary
        self.dictionary_reverse = {value:item for item, value in dictionary.items()}

    def encode(self, text):
        split = re.split(r'([,.:;?_!"()\']|--|\s)', text)
        tokens = list()
        for item in split:
            try: 
                tokens.append(self.dictionary[item])
            except:
                tokens.append(self.dictionary["<|unk|>"])
                
        return tokens

    def decode(self, tokens):
        text = "".join([self.dictionary_reverse[token] for token in tokens])
        return text


tokenizer = SimpleTockenizer(dictionary)
tokens = tokenizer.encode(x)
print(tokens[:10])

text = tokenizer.decode(tokens)
print(text[:100])





tiktok = tiktoken.get_encoding("gpt2")
integers = tiktok.encode(x, allowed_special={"<|eos|>"})
print(integers[:50])

strings = tiktok.decode(integers[:50])
print(strings)





context_size = 4
for i in range(1, context_size+1):
    inputs = integers[:i]
    target = integers[i]
    print(tiktok.decode(inputs) + '------->' + tiktok.decode([target]))
    





class DataLoader:
    """
    A custom data loader which loads data using the tiktoken tokenizer
    into Tensorflow, creating input and target values using the windowing technique
    """
    def __init__(self, text, stride, max_length):
        self.input_ids = []
        self.target_ids = []

        tokenizer = tiktoken.get_encoding("gpt2")
        token_ids = tokenizer.encode(text, allowed_special={"<|eos|>"})
        
        for i in range(0, len(token_ids) - max_length, stride):
            self.input_ids.append(token_ids[i:i+max_length])
            self.target_ids.append(token_ids[i+1:i+1+max_length])
            
        # Convert lists to TensorFlow tensors
        self.input_ids = tf.convert_to_tensor(self.input_ids, dtype=tf.int32)
        self.target_ids = tf.convert_to_tensor(self.target_ids, dtype=tf.int32)
            
    def __len__(self):
        return len(self.input_ids)
        
    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]
    
    def load_data(self, batch_size=8, shuffle=False):
        dataset = tf.data.Dataset.from_tensor_slices((self.input_ids, self.target_ids)) 
        # Shuffle the dataset if required
        if shuffle:
            dataset = dataset.shuffle(buffer_size=buffer_size)

        # Batch the dataset
        dataset = dataset.batch(batch_size)

        # Prefetch the dataset for better performance
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

        return dataset


data = DataLoader(x, 2, 10)
dataset = data.load_data(batch_size=1)


print(next(iter(dataset)))





max_length = 100
stride = 4
batch_size = 8

data = DataLoader(x, stride, max_length)
dataset = data.load_data(batch_size)


vocab_size = 50257
output_dim = 256
context_len = max_length

token_embedding = keras.layers.Embedding(vocab_size, output_dim)(next(iter(dataset))[0])
# print(embedding(next(iter(dataset))[0].numpy()))


pos_idx = tf.range(context_len)
pos_embedding = keras.layers.Embedding(context_len, output_dim)(pos_idx)
print(pos_embedding)

input_embedding = token_embedding + pos_embedding





def Attention(inputs):
    """
    A very simple implementation of the Self Attention Layer
    """

    # 1. Calculate the relationship between each input and all other inputs in the sequence
    attention_scores = tf.matmul(inputs, tf.transpose(inputs))

    # 2. Normalize the attention scores for better learning (better for gradient descent)
    norm_as = keras.layers.Softmax()(attention_scores)

    # 3. general the final context vector by multiplying each attention score with its corresponding input and suming them up
    final_context_vec = tf.matmul(norm_as, inputs)

    return final_context_vec



input_embedding.shape



class SelfAttention(keras.Layer):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def build(self, input_shape):
        self.Wq = self.add_weight((input_shape[-1], self.dim), name="Wq")
        self.Wk = self.add_weight((input_shape[-1], self.dim), name="Wk")
        self.Wv = self.add_weight((input_shape[-1], self.dim), name="Wv")

    def call(self, inputs):
        self.keys = K.dot(self.Wk, inputs)
        self.queries = K.dot(self.Wq, inputs)
        self.values = K.dot(self.Wv, inputs)

        self.attention_score = K.dot(K.T)



